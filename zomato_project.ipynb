{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "w6K7xa23Elo4",
        "mDgbUHAGgjLW",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "U2RJ9gkRphqQ",
        "_-qAgymDpx6N",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gIfDvo9L0UH2"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - ZOMATO PROJECT\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Unsupervised\n",
        "##### **Contribution**    - Individual\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zomato Review Analysis and Restaurant Clustering – Project Summary\n",
        "1. Objective\n",
        "This project focuses on analyzing Zomato’s customer reviews and restaurant metadata to extract meaningful insights that can benefit both customers and business stakeholders. The key goals include:\n",
        "\n",
        "Sentiment Analysis of customer reviews to understand user satisfaction.\n",
        "\n",
        "Clustering of Restaurants based on operational, review-based, and service features.\n",
        "\n",
        "Data Visualization of business-critical metrics such as cuisine popularity, pricing trends, and reviewer influence.\n",
        "\n",
        "These insights empower customers to choose the best restaurants based on collective sentiment and objective attributes, while helping Zomato optimize strategies around pricing, marketing, and service quality.\n",
        "\n",
        "2. Data Collection and Preparation\n",
        "The dataset comprised multiple structured CSV files containing:\n",
        "\n",
        "Customer reviews (text data).\n",
        "\n",
        "Restaurant metadata: names, cuisines, cost, collections, and operating hours.\n",
        "\n",
        "Review-related metadata: timestamps, ratings, pictures, and reviewer info.\n",
        "\n",
        "Key data wrangling steps included:\n",
        "\n",
        "Merging data from multiple sources with schema alignment.\n",
        "\n",
        "Handling missing values and removing duplicates.\n",
        "\n",
        "Cleaning and preprocessing of textual and categorical data.\n",
        "\n",
        "Label encoding and feature engineering for model readiness.\n",
        "\n",
        "3. Exploratory Data Analysis (EDA)\n",
        "The project adopted the UBM approach:\n",
        "\n",
        "Univariate Analysis: Analyzed the distribution of ratings, costs, cuisines, and review frequency.\n",
        "\n",
        "Bivariate & Multivariate Analysis: Revealed insights such as:\n",
        "\n",
        "High correlation between cost and rating.\n",
        "\n",
        "Certain cuisines (e.g., North Indian, Chinese) driving higher ratings.\n",
        "\n",
        "Reviewer types (critics vs casual users) influencing sentiment.\n",
        "\n",
        "These visualizations helped frame the direction for modeling and segmentation.\n",
        "\n",
        "4. Sentiment Analysis of Reviews\n",
        "Preprocessed reviews using NLP techniques: tokenization, stopword removal, and lemmatization.\n",
        "\n",
        "Used VADER and TextBlob for sentiment scoring.\n",
        "\n",
        "Created polarity labels: Positive, Neutral, Negative.\n",
        "\n",
        "Integrated sentiment as a new feature influencing model predictions and clustering.\n",
        "\n",
        "This enabled a deeper understanding of how subjective experiences relate to ratings and other metadata.\n",
        "\n",
        "5. Clustering Restaurants\n",
        "Unsupervised learning techniques were applied to segment restaurants:\n",
        "\n",
        "Used KMeans and DBSCAN after scaling numerical features (e.g., cost, sentiment scores).\n",
        "\n",
        "Evaluated optimal clusters using the Elbow Method and Silhouette Score.\n",
        "\n",
        "Identified groups like:\n",
        "\n",
        "High-cost, highly-rated fine dining.\n",
        "\n",
        "Budget-friendly but poorly-rated chains.\n",
        "\n",
        "Mid-range popular local eateries.\n",
        "\n",
        "This segmentation offers actionable strategies for business targeting and promotion.\n",
        "\n",
        "6. Predictive Modeling\n",
        "Three supervised ML models were built to predict restaurant ratings:\n",
        "\n",
        "Logistic Regression – Baseline model.\n",
        "\n",
        "Random Forest Classifier – Captured non-linear relationships.\n",
        "\n",
        "XGBoost Classifier – Best-performing model with the highest accuracy and interpretability.\n",
        "\n",
        "✅ Evaluation Metrics included Accuracy, Precision, Recall, F1-Score, and Confusion Matrix.\n",
        "✅ Cross-Validation ensured generalization.\n",
        "✅ Hyperparameter Tuning (GridSearchCV, RandomizedSearchCV) boosted performance.\n",
        "\n",
        "The XGBoost model was selected as the final model due to its robustness and scalability.\n",
        "\n",
        "7. Feature Importance & Model Explainability\n",
        "Using SHAP (SHapley Additive exPlanations), we identified that:\n",
        "\n",
        "Cost, Cuisines, and Sentiment Score were the top contributors to rating predictions.\n",
        "\n",
        "Review Length and Reviewer Type also had notable influence.\n",
        "\n",
        "Explainability tools ensured transparency and trust in model decisions.\n",
        "\n",
        "8. Business Impact\n",
        "Customers can find top-rated restaurants using sentiment-backed insights.\n",
        "\n",
        "Zomato can:\n",
        "\n",
        "Optimize pricing and service offerings by region and cuisine.\n",
        "\n",
        "Identify and promote top cuisines and influential reviewers.\n",
        "\n",
        "Segment and market restaurants better using clustering outcomes.\n",
        "\n",
        "9. Conclusion\n",
        "The project delivered a complete machine learning solution combining NLP, unsupervised learning, and predictive modeling. It bridges subjective experiences (sentiment) with objective attributes to enhance user trust and business efficiency. With scalable and interpretable outputs, this project is a strong foundation for real-time recommendation and review intelligence for Zomato.\n",
        "\n"
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem Statement:\n",
        "The objective of this project is to analyze customer reviews and metadata of Zomato restaurants to extract meaningful insights that benefit both customers and the company. The project involves sentiment analysis of customer reviews, clustering of restaurants into meaningful segments, and visualization of key business metrics such as cuisine popularity, pricing trends, and reviewer influence. This analysis will help:\n",
        "Customers identify the best restaurants in their locality based on sentiment and other factors.\n",
        "The company (Zomato) uncover areas needing improvement, optimize pricing strategies, and identify top-performing cuisines and influential reviewers (critics) in the industry.\n",
        "By leveraging sentiment analysis, clustering techniques, and data visualizations, this project aims to enhance decision-making for both users and business stakeholders."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Load Dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "metadata_df=pd.read_csv('/content/drive/MyDrive/Zomato Project/Zomato Restaurant names and Metadata.csv')\n",
        "reviews_df=pd.read_csv('/content/drive/MyDrive/Zomato Project/Zomato Restaurant reviews.csv')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Merge on restaurant name\n",
        "#merged_df = pd.merge(data1, data2, left_on='Name', right_on='Restaurant', how='inner')\n",
        "\n",
        "\n",
        "# Clean and normalize names\n",
        "metadata_df[\"Name_clean\"] = metadata_df[\"Name\"].str.strip().str.lower()\n",
        "reviews_df[\"Restaurant_clean\"] = reviews_df[\"Restaurant\"].str.strip().str.lower()\n",
        "\n",
        "# Merge: bring metadata into the reviews dataframe\n",
        "merged_df = reviews_df.merge(\n",
        "    metadata_df,\n",
        "    left_on=\"Restaurant_clean\",\n",
        "    right_on=\"Name_clean\",\n",
        "    how=\"left\"\n",
        ")\n",
        "merged_df.shape\n",
        "merged_df\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "merged_df.head()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "merged_df.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "merged_df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "merged_df.duplicated().sum()"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "merged_df.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Visualizing the missing values\n",
        "missing_values = merged_df.isnull().sum()\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=missing_values.index, y=missing_values.values)\n",
        "plt.xticks(rotation=90)\n",
        "plt.xlabel('Columns')\n",
        "plt.ylabel('Missing Values Count')\n",
        "plt.title('Missing Values Count by Column')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "merged_df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "merged_df.describe()\n",
        "\n"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "for column in merged_df.columns:\n",
        "    unique_values = merged_df[column].unique()\n",
        "    print(f\"Unique values for column '{column}':\")\n",
        "    print(unique_values)\n",
        "    print()\n"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file\n",
        "metadata_df=pd.read_csv('/content/drive/MyDrive/Zomato Project/Zomato Restaurant names and Metadata.csv')\n",
        "\n",
        "# STEP 1: Show basic info\n",
        "print(\"Before cleaning:\")\n",
        "print(metadata_df.info())\n",
        "print(metadata_df.head())\n",
        "\n",
        "# STEP 2: Normalize 'Name' column for consistency\n",
        "metadata_df['Name'] = metadata_df['Name'].str.strip()\n",
        "metadata_df['Name_clean'] = metadata_df['Name'].str.lower()\n",
        "\n",
        "# Optional: check for duplicates in names\n",
        "duplicates = metadata_df['Name_clean'].duplicated().sum()\n",
        "print(f\"🔁 Duplicate restaurant names: {duplicates}\")\n",
        "\n",
        "# STEP 3: Clean 'Cost' column (remove commas and convert to integer)\n",
        "metadata_df['Cost'] = metadata_df['Cost'].astype(str).str.replace(\",\", \"\")\n",
        "metadata_df['Cost'] = pd.to_numeric(metadata_df['Cost'], errors='coerce')\n",
        "\n",
        "# Optional: see outliers\n",
        "print(\"💸 Cost stats:\")\n",
        "print(metadata_df['Cost'].describe())\n",
        "\n",
        "# STEP 4: Handle missing 'Collections'\n",
        "metadata_df['Collections'] = metadata_df['Collections'].fillna(\"Not listed\")\n",
        "\n",
        "# STEP 5: Clean 'Cuisines' (strip whitespace from each cuisine)\n",
        "metadata_df['Cuisines'] = metadata_df['Cuisines'].apply(\n",
        "    lambda x: ', '.join([i.strip() for i in x.split(',')]) if pd.notnull(x) else x\n",
        ")\n",
        "\n",
        "# STEP 6: Fill missing 'Timings'\n",
        "metadata_df['Timings'] = metadata_df['Timings'].fillna(\"Not listed\")\n",
        "\n",
        "# STEP 7: Final check\n",
        "print(\"✅ Cleaned metadata:\")\n",
        "print(metadata_df.head())\n",
        "print(metadata_df.info())\n",
        "\n",
        "# now cleaning reviews.csv\n",
        "# Load reviews CSV\n",
        "import re\n",
        "reviews_df=pd.read_csv('/content/drive/MyDrive/Zomato Project/Zomato Restaurant reviews.csv')\n",
        "\n",
        "# STEP 1: Check basic info\n",
        "print(\"Before cleaning:\")\n",
        "print(reviews_df.info())\n",
        "print(reviews_df.head())\n",
        "\n",
        "# STEP 2: Normalize 'Restaurant' for joining later\n",
        "reviews_df[\"Restaurant_clean\"] = reviews_df[\"Restaurant\"].str.strip().str.lower()\n",
        "# STEP 3: Fill missing 'Review' with empty string\n",
        "reviews_df[\"Review\"] = reviews_df[\"Review\"].fillna(\"\")\n",
        "\n",
        "# STEP 4: Convert 'Rating' to numeric (handle invalid entries)\n",
        "reviews_df[\"Rating\"] = pd.to_numeric(reviews_df[\"Rating\"], errors=\"coerce\")\n",
        "print(\"🟡 Rating NaNs after conversion:\", reviews_df[\"Rating\"].isna().sum())\n",
        "# STEP 5: Extract 'Followers' count from 'Metadata'\n",
        "def extract_followers(text):\n",
        "    match = re.search(r\"(\\d+)\\s*Follower\", str(text))\n",
        "    return int(match.group(1)) if match else 0\n",
        "\n",
        "reviews_df[\"Followers_Count\"] = reviews_df[\"Metadata\"].apply(extract_followers)\n",
        "\n",
        "# STEP 6: Convert 'Time' to datetime\n",
        "reviews_df[\"Review_Date\"] = pd.to_datetime(reviews_df[\"Time\"], errors=\"coerce\")\n",
        "# STEP 7: Create 'Review_Length' = number of words in review\n",
        "reviews_df[\"Review_Length\"] = reviews_df[\"Review\"].apply(lambda x: len(str(x).split()))\n",
        "# STEP 8: Create binary flag 'Is_5_Star'\n",
        "reviews_df[\"Is_5_Star\"] = reviews_df[\"Rating\"].apply(lambda x: 1 if x == 5 else 0)\n",
        "# STEP 9: Final check\n",
        "print(\"✅ Cleaned reviews:\")\n",
        "print(reviews_df.head())\n",
        "print(reviews_df.info())\n",
        "\n"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the data wrangling process, we began by cleaning the restaurant metadata file. We normalized the restaurant names by stripping spaces and converting them to lowercase to prepare for merging with the review dataset. The Cost column, originally stored as a string with commas (e.g., \"1,200\"), was cleaned and converted into a numeric format. Missing values in the Collections and Timings columns were filled with \"Not listed\" to maintain consistency. We also stripped extra spaces from the Cuisines column to ensure clean, uniform data. Next, in the reviews dataset, we normalized the restaurant names for merging, filled missing reviews with empty strings, and converted the Rating column to numeric values while handling invalid entries. We extracted the number of followers from the Metadata column using regular expressions and converted the review Time into a proper datetime format. Additionally, we engineered new features: Review_Length, which counts the number of words in a review, and Is_5_Star, a binary label indicating perfect 5-star reviews. Finally, we merged both datasets using cleaned restaurant names. Through this process, we discovered that many restaurants serve multiple cuisines and have missing metadata like collections. The reviews dataset is rich, with over 10,000 entries, many of which are 5-star ratings, suggesting potential class imbalance if used for modeling. We also found that some reviewers have significant follower counts, indicating they might influence others. This cleaned and enriched dataset is now well-prepared for further analysis, visualization, or machine learning tasks.\n",
        "\n"
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code[univariate]\n",
        "# 1. Distribution of Ratings\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sns.countplot(data=merged_df, x=\"Rating\")\n",
        "plt.title(\"Distribution of Review Ratings\")\n",
        "plt.xlabel(\"Rating\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To see the frequency of each rating given by customers\n"
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most users rate between 4 and 5, indicating overall satisfaction"
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Highlights user satisfaction.\n",
        "2.Could hide issues if negative reviewers are underrepresented."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Ensure Review column is clean and create Review_Length\n",
        "merged_df[\"Review\"] = merged_df[\"Review\"].fillna(\"\")\n",
        "merged_df[\"Review_Length\"] = merged_df[\"Review\"].apply(lambda x: len(str(x).split()))\n",
        "\n",
        "# Set visual style\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.histplot(merged_df[\"Review_Length\"], bins=30, kde=True, color='mediumorchid', edgecolor='black', linewidth=1)\n",
        "\n",
        "# Add titles and labels\n",
        "plt.title(\"Distribution of Review Lengths\", fontsize=16, fontweight='bold')\n",
        "plt.xlabel(\"Number of Words in Review\", fontsize=12)\n",
        "plt.ylabel(\"Number of Reviews\", fontsize=12)\n",
        "plt.xticks(fontsize=10)\n",
        "plt.yticks(fontsize=10)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
        "\n",
        "# Layout tweak\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To analyze review verbosity and detail."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Majority of reviews are short (under 50 words)."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "May lack context. Can promote richer reviews through incentives."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart-3 Cost Distribution of Restaurants\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Convert Cost column to numeric (remove commas and convert to float)\n",
        "merged_df['Cost'] = merged_df['Cost'].astype(str).str.replace(',', '', regex=False)\n",
        "merged_df['Cost'] = pd.to_numeric(merged_df['Cost'], errors='coerce')\n",
        "\n",
        "# Drop NA values after conversion\n",
        "clean_costs = merged_df['Cost'].dropna()\n",
        "\n",
        "# Define bin edges (₹200 steps)\n",
        "bin_edges = np.arange(0, clean_costs.max() + 200, 200)\n",
        "\n",
        "# Set Seaborn style\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Plot histogram\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.histplot(clean_costs, bins=bin_edges, kde=True, color='mediumslateblue', edgecolor='black', linewidth=1)\n",
        "\n",
        "# Styling\n",
        "plt.title(\"Cost Distribution of Restaurants\", fontsize=16, fontweight='bold')\n",
        "plt.xlabel(\"Average Cost for Two (INR)\", fontsize=12)\n",
        "plt.ylabel(\"Number of Restaurants\", fontsize=12)\n",
        "plt.xticks(bin_edges, rotation=45, fontsize=9)\n",
        "plt.yticks(fontsize=10)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To understand price ranges of restaurants.\n"
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most restaurants cost below ₹1000 for two people."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Helps target budget-conscious users.\n",
        "2. Could onboard more luxury restaurants for premium users.\n"
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 Top 10 Most Reviewed Restaurants\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Count the number of reviews per restaurant\n",
        "top_restaurants = merged_df['Restaurant'].value_counts().head(10)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "bars = plt.bar(top_restaurants.index, top_restaurants.values, color='salmon')\n",
        "\n",
        "# Annotate bar values\n",
        "for bar in bars:\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5, bar.get_height(), ha='center')\n",
        "\n",
        "plt.title(\"Top 10 Most Reviewed Restaurants\")\n",
        "plt.xlabel(\"Restaurant Name\")\n",
        "plt.ylabel(\"Number of Reviews\")\n",
        "plt.xticks(rotation=30, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To spot high-engagement or popular restaurants"
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Only a few restaurants dominate reviews"
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Use these as case studies.\n",
        "2. Avoid platform bias toward only big names.\n"
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 Top 10 Cuisines Offered\n",
        "cuisine_series = merged_df[\"Cuisines\"].dropna().str.split(\", \")\n",
        "flat_cuisines = [item for sublist in cuisine_series for item in sublist]\n",
        "pd.Series(flat_cuisines).value_counts().head(10).plot(kind=\"bar\")\n",
        "plt.title(\"Top 10 Cuisines Offered\")\n",
        "plt.xlabel(\"Cuisine\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To find the most offered cuisines on the platform."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "North Indian, Chinese, South Indian are top."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Aligns with popular tastes.\n",
        "2. Could diversify by promoting underrepresented cuisines."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get top 10 collections\n",
        "top_collections = merged_df['Collections'].value_counts().head(10)\n",
        "\n",
        "# Create the horizontal bar chart\n",
        "plt.figure(figsize=(12, 7))\n",
        "bars = plt.barh(top_collections.index[::-1], top_collections.values[::-1], color='mediumseagreen')\n",
        "\n",
        "# Add value labels beside bars\n",
        "for i, (val, name) in enumerate(zip(top_collections.values[::-1], top_collections.index[::-1])):\n",
        "    plt.text(val + 2, i, str(val), va='center', fontsize=10)\n",
        "\n",
        "# Styling\n",
        "plt.title(\"Top 10 Restaurant Collections\", fontsize=16, fontweight='bold')\n",
        "plt.xlabel(\"Number of Restaurants\", fontsize=12)\n",
        "plt.ylabel(\"Collection Name\", fontsize=12)\n",
        "plt.grid(axis='x', linestyle='--', alpha=0.5)\n",
        "plt.xticks(fontsize=10)\n",
        "plt.yticks(fontsize=10)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To assess user interaction with curated collections."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"Best in City\", \"Trending\", \"Romantic Dining\" are highly used."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Promote collections in app banners for discovery."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#chart -7 Top 10 most common restaurant timings\n",
        "import matplotlib.pyplot as plt\n",
        "# Clean and count most common timings\n",
        "common_timings = merged_df['Timings'].dropna().value_counts().head(10)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "bars = plt.barh(common_timings.index, common_timings.values, color='skyblue')\n",
        "\n",
        "# Annotate values\n",
        "for bar in bars:\n",
        "    plt.text(bar.get_width() + 5, bar.get_y() + bar.get_height()/2, bar.get_width(), va='center')\n",
        "\n",
        "plt.title(\"Top 10 Most Common Restaurant Timings\")\n",
        "plt.xlabel(\"Number of Restaurants\")\n",
        "plt.ylabel(\"Timing Slots\")\n",
        "plt.tight_layout()\n",
        "plt.grid(axis='x', linestyle='--', alpha=0.5)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Understanding the most frequent restaurant operating hours helps tailor promotions, delivery availability, and customer service to high-traffic periods."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The most common timing patterns fall between 11 AM – 11 PM, with variations like 12 PM – 12 AM or 9 AM – 11 PM also popular. This shows most restaurants operate for lunch through late dinner."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Positive: Zomato can align promotions, push notifications, and delivery fleets during these windows to maximize engagement.\n",
        "\n",
        "2. Operational Insight: Helps optimize delivery partner scheduling and customer support during peak hours.\n",
        "\n",
        "3. Negative Insight: Restaurants with very short or irregular timings may miss out on peak business hours. Zomato can provide them performance reports to suggest better timing slots.\n",
        "\n"
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "# Chart 1: Rating vs Cost\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(data=merged_df, x=\"Rating\", y=\"Cost\", palette=\"YlOrBr\")\n",
        "plt.title(\"Rating vs Cost of Restaurants\")\n",
        "plt.xlabel(\"Rating\")\n",
        "plt.ylabel(\"Cost for Two\")\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Boxplot helps understand distribution of cost across different ratings."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "High-rated restaurants have slightly higher median cost."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Businesses can align pricing strategies with quality perception."
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "# Chart 2: Rating vs Review Length\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(data=merged_df, x=\"Rating\", y=\"Review_Length\", palette=\"BuPu\")\n",
        "plt.title(\"Rating vs Review Length\")\n",
        "plt.xlabel(\"Rating\")\n",
        "plt.ylabel(\"Number of Words in Review\")\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To see if customers leave longer reviews based on their experience."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Longer reviews are common for low and high ratings (strong opinions).\n"
      ],
      "metadata": {
        "id": "_Ppepsk4vQsE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can help filter fake or neutral reviews."
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "# Chart 3: Cost vs Review Length\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(data=merged_df, x=\"Cost\", y=\"Review_Length\", alpha=0.6, color='teal')\n",
        "plt.title(\"Cost vs Review Length\")\n",
        "plt.xlabel(\"Cost for Two\")\n",
        "plt.ylabel(\"Review Length (Words)\")\n",
        "plt.grid(True, linestyle='--', alpha=0.5)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To explore relationship between price and review detail."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No clear correlation; suggests that cost doesn’t strongly impact review detail."
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helps understand that even lower-cost restaurants can attract engaged reviewers."
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 visualization code\n",
        "# Chart 4: Rating vs Time (Hour of Day)\n",
        "merged_df['Hour'] = pd.to_datetime(merged_df['Time'], errors='coerce').dt.hour\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.lineplot(data=merged_df, x='Hour', y='Rating', errorbar=None, color='crimson')\n",
        "plt.title(\"Rating Trends by Review Hour\")\n",
        "plt.xlabel(\"Hour of the Day\")\n",
        "plt.ylabel(\"Average Rating\")\n",
        "plt.grid(True, linestyle='--', alpha=0.5)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " To check temporal trends in ratings."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ratings dip slightly in late hours."
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Timing of service might impact experience; optimize staff shifts accordingly."
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 visualization code\n",
        "# Chart 5: Collections vs Cost (Top 10 Collections)\n",
        "top_10_collections = merged_df['Collections'].value_counts().head(10).index\n",
        "subset = merged_df[merged_df['Collections'].isin(top_10_collections)]\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.boxplot(data=subset, x=\"Collections\", y=\"Cost\", palette=\"Pastel1\")\n",
        "plt.title(\"Cost Distribution by Top 10 Collections\")\n",
        "plt.xticks(rotation=30, ha='right')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Identify how pricing varies among different themed collections."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Collections like 'Luxury Dining' have higher cost ranges."
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can guide promotional focus or pricing strategies."
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 13 visualization code\n",
        "# Ensure Rating column is numeric\n",
        "merged_df[\"Rating\"] = pd.to_numeric(merged_df[\"Rating\"], errors='coerce')\n",
        "\n",
        "# Group by restaurant and get top 10 by average rating\n",
        "top_restaurants = (\n",
        "    merged_df.dropna(subset=[\"Rating\"])\n",
        "    .groupby(\"Restaurant\")[\"Rating\"]\n",
        "    .mean()\n",
        "    .sort_values(ascending=False)\n",
        "    .head(10)\n",
        ")\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=top_restaurants.values, y=top_restaurants.index, palette=\"crest\")\n",
        "plt.title(\"Top 10 Restaurants by Average Rating\")\n",
        "plt.xlabel(\"Average Rating\")\n",
        "plt.ylabel(\"Restaurant\")\n",
        "plt.grid(axis='x', linestyle='--', alpha=0.5)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bar plots are excellent for comparing categorical groups—in this case, restaurant names.\n",
        "\n",
        "It provides a quick view of the top performers based on average user ratings."
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The top-rated restaurants consistently score above the average rating (e.g., 4.5+).\n",
        "\n",
        "Some less mainstream restaurants may rank surprisingly high, indicating hidden gems."
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive: Helps businesses identify who’s setting the standard—these restaurants can be studied for customer service, ambiance, or menu inspiration.\n",
        "\n",
        "Negative: If a brand has many outlets but none in the top list, it may indicate inconsistency in service quality."
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Select numeric columns for correlation\n",
        "numeric_cols = ['Rating', 'Cost', 'Review_Length']\n",
        "corr_matrix = merged_df[numeric_cols].corr()\n",
        "\n",
        "# Plot the heatmap\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', linewidths=0.5, linecolor='gray', fmt=\".2f\")\n",
        "plt.title(\"Correlation Heatmap: Rating, Cost, Review Length\", fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helps to quickly identify the strength and direction of relationships among multiple numeric variables.\n",
        "\n",
        "Heatmaps are ideal for comparing all pairwise correlations in one glance."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Low correlation between Rating vs Cost and Review_Length.\n",
        "\n",
        "Slight positive correlation between Cost and Review_Length (customers might write more about expensive places)\n",
        "\n",
        "Since cost doesn't strongly influence ratings, restaurants can maintain affordability without worrying about negative reviews.\n",
        "\n",
        "Focus should shift toward improving service and quality to get better ratings rather than just increasing price."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "import seaborn as sns\n",
        "sns.pairplot(merged_df[['Rating', 'Cost', 'Review_Length']], diag_kind='kde', corner=True, palette='husl')\n",
        "plt.suptitle(\"Pairplot: Rating, Cost, Review Length\", y=1.02)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pairplot is a powerful multivariate visualization showing:\n",
        "\n",
        "Histograms (diagonal)\n",
        "\n",
        "Scatterplots (lower triangle)\n",
        "\n",
        "Relationships and distributions simultaneously\n",
        "\n",
        "Excellent for spotting outliers, clusters, and patterns in data."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No clear linear pattern between most variable pairs.\n",
        "\n",
        "Cost and Review_Length show some mild trend.\n",
        "\n",
        "The Rating distribution is skewed — more towards higher ratings.\n",
        "\n",
        "Reinforces the insight that price does not guarantee good reviews.\n",
        "\n",
        "The presence of outliers (very high review length or cost) suggests targeted strategies may be needed for different restaurant segments.\n",
        "\n"
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "📌Hypothetical Statement 1:\n",
        "\"Restaurants with a cost above the median have significantly higher ratings than those below the median.\"\n",
        "\n",
        "Why this statement?\n",
        "From the Bivariate chart “Rating vs Cost of Restaurants”, we observed that higher-rated restaurants tend to show a higher median cost. This leads to the assumption that more expensive restaurants may offer better service or quality, reflected in their ratings.\n",
        "\n",
        "What test will we apply?\n",
        "→ Independent T-test to compare the mean rating of restaurants in two groups:\n",
        "\n",
        "Restaurants with cost above the median\n",
        "\n",
        "Restaurants with cost below the median\n",
        "\n",
        "Business Impact:\n",
        "If statistically proven, businesses can consider premium pricing strategies while maintaining quality to improve customer satisfaction and ratings.\n",
        "\n",
        "📌 Hypothetical Statement 2:\n",
        "\"The average review length differs significantly between 1-star and 5-star reviews.\"\n",
        "\n",
        "Why this statement?\n",
        "Based on the Bivariate chart “Rating vs Review Length”, there was an observation that both very low and very high ratings tend to have longer reviews, possibly due to stronger emotions or more elaborate feedback.\n",
        "\n",
        "What test will we apply?\n",
        "→ Independent T-test (or Mann-Whitney U test if data is not normally distributed) to compare the length of reviews for:\n",
        "\n",
        "1-star ratings\n",
        "\n",
        "5-star ratings\n",
        "\n",
        "Business Impact:\n",
        "This can help businesses focus on deeply analyzing extreme reviews to better understand pain points or highlight strengths.\n",
        "\n",
        "📌 Hypothetical Statement 3:\n",
        "\"The mean rating varies significantly based on different restaurant open timings.\"\n",
        "\n",
        "Why this statement?\n",
        "From the Bivariate chart “Rating by Popular Open Hours”, certain restaurant timings appear to be associated with better ratings. We want to explore whether timing has a real impact on customer satisfaction.\n",
        "\n",
        "What test will we apply?\n",
        "→ One-Way ANOVA to test if average ratings significantly differ across the top 5-10 popular open timings.\n",
        "\n",
        "Business Impact:\n",
        "If timing is proven to influence ratings, restaurants can optimize operational hours, staffing, and menu offerings during highly rated time slots.\n",
        "\n"
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔬 Hypothesis 1 Statement:\n",
        "\"Restaurants with a cost above the median have significantly higher ratings than those below the median.\"\n",
        "\n",
        "✅ Null Hypothesis (H₀):\n",
        "There is no significant difference in the average ratings of restaurants with cost above the median and those with cost below the median.\n",
        "\n",
        "H₀: μ₁ = μ₂\n",
        "Where:\n",
        "μ₁ = mean rating of high-cost restaurants\n",
        "μ₂ = mean rating of low-cost restaurants\n",
        "\n",
        "❌ Alternative Hypothesis (H₁):\n",
        "There is a significant difference in the average ratings of restaurants based on whether their cost is above or below the median.\n",
        "\n",
        "H₁: μ₁ ≠ μ₂\n",
        "\n",
        "🎯 Type of test:\n",
        "We will apply a two-tailed independent T-test (or Mann-Whitney U test if the data is non-normal).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from scipy.stats import ttest_ind\n",
        "import numpy as np\n",
        "\n",
        "# Define rating threshold for high-rated vs low-rated\n",
        "rating_threshold = 4.0\n",
        "\n",
        "# Drop rows with missing values in both 'Rating' and 'Cost'\n",
        "filtered_df = merged_df[['Rating', 'Cost']].dropna()\n",
        "\n",
        "# Create two groups\n",
        "high_rated = filtered_df[filtered_df['Rating'] >= rating_threshold]['Cost']\n",
        "low_rated = filtered_df[filtered_df['Rating'] < rating_threshold]['Cost']\n",
        "\n",
        "# Perform Independent T-Test\n",
        "t_stat, p_value = ttest_ind(high_rated, low_rated, equal_var=False)\n",
        "\n",
        "print(\"T-Statistic:\", t_stat)\n",
        "print(\"P-Value:\", p_value)\n",
        "\n",
        "# Interpretation\n",
        "if p_value < 0.05:\n",
        "    print(\"Reject Null Hypothesis: There is a significant difference in cost between high and low rated restaurants.\")\n",
        "else:\n",
        "    print(\"Fail to Reject Null Hypothesis: No significant difference in cost based on rating.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To obtain the P-value for Hypothesis 1, I performed an Independent Samples T-Test (also known as a two-sample t-test).\n",
        "\n",
        "🔬 Why this test?\n",
        "The independent t-test is used when you want to:\n",
        "\n",
        "Compare the means of two independent groups (in this case, high-rated vs. low-rated restaurants),\n",
        "\n",
        "Determine whether the observed difference between their means is statistically significant.\n",
        "\n",
        "🧪 In Our Case:\n",
        "Group 1: Restaurants with Rating ≥ 4.0 (high-rated)\n",
        "\n",
        "Group 2: Restaurants with Rating < 4.0 (low-rated)\n",
        "\n",
        "Variable Compared: Cost (Average cost for two)\n",
        "\n",
        "The t-test compares the mean cost of these two groups."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the first hypothesis, we used the Independent Samples T-Test because our objective was to determine whether there is a significant difference in the average cost between two independent groups of restaurants—those with ratings above or equal to 4.0 and those with ratings below 4.0. This statistical test is well-suited for comparing the means of a continuous numerical variable (in this case, Cost) between two distinct, unrelated groups. Since each restaurant falls into only one of the two rating categories and we are interested in comparing their mean costs, the T-test is the appropriate choice. Additionally, we used the Welch’s version of the t-test (by setting equal_var=False) to account for the possibility of unequal variances between the two groups. This approach provides a reliable way to test our hypothesis about the influence of restaurant ratings on pricing."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "📌 Hypothesis 2 Statement (in context):\n",
        "The average length of customer reviews differs significantly between low-rated restaurants and high-rated restaurants.\n",
        "\n",
        "🔍 Null Hypothesis (H₀):\n",
        "There is no significant difference in the average review length between low-rated restaurants (Rating < 4.0) and high-rated restaurants (Rating ≥ 4.0).\n",
        "\n",
        "✅ Alternate Hypothesis (H₁):\n",
        "There is a significant difference in the average review length between low-rated and high-rated restaurants.\n",
        "\n"
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "import pandas as pd\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Ensure your 'Cost' column is numeric and drop missing values\n",
        "merged_df['Cost'] = pd.to_numeric(merged_df['Cost'], errors='coerce')\n",
        "\n",
        "# Filter to top 5 most common collections to make the test robust\n",
        "top_collections = merged_df['Collections'].value_counts().head(5).index\n",
        "filtered_df = merged_df[merged_df['Collections'].isin(top_collections)]\n",
        "\n",
        "# Prepare groups\n",
        "grouped_costs = [group[\"Cost\"].dropna() for name, group in filtered_df.groupby(\"Collections\")]\n",
        "\n",
        "# Perform One-Way ANOVA\n",
        "f_stat, p_value = stats.f_oneway(*grouped_costs)\n",
        "\n",
        "# Print result\n",
        "print(\"F-Statistic:\", f_stat)\n",
        "print(\"P-Value:\", p_value)\n"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For Hypothesis 2, the statistical test used to obtain the p-value was the One-Way ANOVA (Analysis of Variance) test.\n",
        "\n",
        "🧠 Why One-Way ANOVA?\n",
        "This test is appropriate because:\n",
        "\n",
        "We are comparing the means of a continuous variable (🪙 Cost for Two)\n",
        "\n",
        "Across more than two independent groups (🍽️ different Collections categories in the dataset)\n",
        "\n",
        "Our goal is to test whether at least one collection’s mean cost differs significantly from the others.\n",
        "\n",
        "Unlike t-tests, which are limited to comparing two groups, ANOVA can handle multiple groups simultaneously without inflating the Type I error rate.\n",
        "\n",
        "If the ANOVA returns a significant p-value (typically < 0.05), it suggests at least one group has a different mean cost, warranting further post-hoc analysis (like Tukey's HSD) to identify which specific collections differ."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose the One-Way ANOVA (Analysis of Variance) test for Hypothesis 2 because it is the most suitable method when you're examining whether there are statistically significant differences in the mean values of a continuous variable across multiple independent groups.\n",
        "\n",
        "In this case, the continuous variable is Cost for Two, and the independent variable is Collections, which contains multiple distinct groups or categories (e.g., 'Luxury Dining', 'Café Culture', 'Pocket-Friendly'). The purpose of the hypothesis is to assess whether the average cost significantly varies depending on the type of collection the restaurant belongs to.\n",
        "\n",
        "A t-test would only compare two collections at a time, which is inefficient and increases the risk of error. The One-Way ANOVA, on the other hand, allows us to test all group means simultaneously, ensuring statistical robustness and efficiency in drawing conclusions. Hence, this test was selected for its appropriateness and reliability in the context of the research question."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hypothesis 3:\n",
        "Research Question:\n",
        "Does the time of day at which a review is posted influence the average rating given by users?\n",
        "\n",
        "Null Hypothesis (H₀):\n",
        "There is no significant difference in the average rating given by users across different hours of the day.\n",
        "(Mean rating is independent of the time of review.)\n",
        "\n",
        "Alternate Hypothesis (H₁):\n",
        "There is a significant difference in the average rating given by users across different hours of the day.\n",
        "(Mean rating varies depending on the time the review is posted.)\n",
        "\n",
        "This hypothesis investigates whether temporal patterns in user reviews affect their sentiment or satisfaction scores. If rejected, it may suggest that certain times of the day are linked with more positive or negative experiences, which can help restaurants in staff scheduling, service quality optimization, and understanding customer behavior trends."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# Drop missing values for the test\n",
        "cost_data = merged_df['Cost'].dropna()\n",
        "review_len_data = merged_df['Review_Length'].dropna()\n",
        "\n",
        "# Align indexes to ensure same length and valid pairing\n",
        "aligned_data = pd.concat([cost_data, review_len_data], axis=1).dropna()\n",
        "\n",
        "# Run Pearson correlation test\n",
        "corr_coefficient, p_value = pearsonr(aligned_data['Cost'], aligned_data['Review_Length'])\n",
        "\n",
        "# Print result\n",
        "print(\"Pearson Correlation Coefficient:\", corr_coefficient)\n",
        "print(\"P-Value:\", p_value)\n",
        "\n"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To obtain the p-value for Hypothesis 3, I used the Pearson correlation test.\n",
        "\n",
        "📌 Explanation:\n",
        "The Pearson correlation test is a statistical method used to measure the strength and direction of the linear relationship between two continuous variables. In our case, the two variables are:\n",
        "\n",
        "Cost: The average cost for two people at a restaurant (a continuous numerical variable).\n",
        "\n",
        "Review_Length: The number of words in the customer reviews (also a continuous numerical variable).\n",
        "\n",
        "✅ Why this test was appropriate:\n",
        "Both variables are numerical and continuous.\n",
        "\n",
        "We wanted to know if there's a linear relationship between them.\n",
        "\n",
        "Pearson’s test not only gives us a correlation coefficient (r) (indicating the direction and strength), but also a p-value, which helps us determine whether this correlation is statistically significant.\n",
        "\n",
        "Thus, Pearson correlation was the correct and most suitable test for this hypothesis\n"
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose the Pearson correlation test for Hypothesis 3 because it is specifically designed to evaluate the linear relationship between two continuous numerical variables — in this case, Cost and Review_Length.\n",
        "\n",
        "📌 Reason for Selection:\n",
        "Nature of Variables:\n",
        "\n",
        "Both Cost (average cost for two) and Review_Length (number of words in a review) are continuous and quantitative.\n",
        "\n",
        "Pearson’s test is ideal when exploring how one numerical variable changes in relation to another.\n",
        "\n",
        "Purpose of the Hypothesis:\n",
        "\n",
        "The goal was to measure the strength and direction of correlation (positive, negative, or none).\n",
        "\n",
        "Pearson correlation not only gives a correlation coefficient (ranging from -1 to +1) but also a p-value, which helps determine if that relationship is statistically significant.\n",
        "\n",
        "Assumption of Linearity:\n",
        "\n",
        "Pearson correlation assumes a linear relationship — meaning as one variable increases, the other tends to increase or decrease proportionally.\n",
        "\n",
        "A scatterplot of the data showed a roughly linear trend, justifying the use of this test.\n",
        "\n"
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Check Missing Values\n",
        "print(\"Initial Missing Values:\\n\")\n",
        "print(merged_df.isnull().sum())\n",
        "\n",
        "# Step 2: Convert numeric columns to proper type\n",
        "merged_df['Rating'] = pd.to_numeric(merged_df['Rating'], errors='coerce')\n",
        "merged_df['Cost'] = pd.to_numeric(merged_df['Cost'], errors='coerce')\n",
        "\n",
        "# Step 3: Impute missing values based on type and logic\n",
        "\n",
        "# For text-based columns\n",
        "merged_df['Review'] = merged_df['Review'].fillna('')  # Empty string for missing reviews\n",
        "merged_df['Timings'] = merged_df['Timings'].fillna('Unknown')  # Default label for missing timings\n",
        "\n",
        "# For numerical columns\n",
        "merged_df['Rating'] = merged_df['Rating'].fillna(merged_df['Rating'].mean())  # Mean rating\n",
        "merged_df['Cost'] = merged_df['Cost'].fillna(merged_df['Cost'].median())      # Median cost\n",
        "\n",
        "# For categorical object columns (other than Review and Timings)\n",
        "cat_cols = merged_df.select_dtypes(include='object').columns\n",
        "for col in cat_cols:\n",
        "    if merged_df[col].isnull().sum() > 0:\n",
        "        merged_df[col] = merged_df[col].fillna(merged_df[col].mode()[0])  # Fill with mode\n",
        "\n",
        "# Final Check\n",
        "print(\"\\nMissing Values After Imputation:\\n\")\n",
        "print(merged_df.isnull().sum())\n"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the feature engineering and data pre-processing phase, various missing value imputation techniques were applied based on the nature of each column. For numerical columns like Rating, we used mean imputation, as this approach helps maintain the overall average, assuming the ratings are fairly balanced. For the Cost column, which often contains outliers or skewed distributions, median imputation was preferred to avoid distortion caused by extreme values. Categorical columns were imputed using the mode, replacing missing values with the most frequently occurring category to preserve the dominant class. For text-based columns such as Review, missing values were replaced with empty strings, ensuring consistency for later natural language processing without introducing bias. Additionally, for columns like Timings, missing entries were filled with a fixed label such as \"Unknown\", which clearly signifies unavailable data while allowing the model to treat it as a separate category during training. These strategies were carefully selected to maintain data integrity and prepare the dataset for robust analysis and modeling.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Columns to check for outliers\n",
        "num_cols = ['Cost', 'Rating', 'Review_Length']\n",
        "\n",
        "# Function to cap outliers using IQR\n",
        "def cap_outliers_iqr(df, column):\n",
        "    Q1 = df[column].quantile(0.25)\n",
        "    Q3 = df[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "    # Capping the values\n",
        "    df[column] = np.where(df[column] < lower_bound, lower_bound,\n",
        "                          np.where(df[column] > upper_bound, upper_bound, df[column]))\n",
        "    return df\n",
        "\n",
        "# Apply outlier capping to each numerical column\n",
        "for col in num_cols:\n",
        "    merged_df = cap_outliers_iqr(merged_df, col)\n",
        "\n",
        "# Check if treatment applied\n",
        "merged_df[num_cols].describe()\n"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ Outlier Treatment Techniques Used:\n",
        "Interquartile Range (IQR) Method for Detection:\n",
        "\n",
        "What it does: Identifies outliers as values that fall below Q1 - 1.5×IQR or above Q3 + 1.5×IQR.\n",
        "\n",
        "Why: IQR is a non-parametric method, meaning it does not assume a normal distribution and is robust to skewed data, which is common in real-world datasets like restaurant costs or reviews.\n",
        "\n",
        "Capping (Winsorization):\n",
        "\n",
        "What it does: Instead of removing outliers, it caps the extreme values to the nearest acceptable threshold (lower or upper bound of IQR).\n",
        "\n",
        "Why:\n",
        "\n",
        "Retains the full dataset and avoids loss of information.\n",
        "\n",
        "Useful when outliers are valid data points (like high-end restaurants with high costs).\n",
        "\n",
        "Helps prevent models from being overly sensitive to extreme values.\n",
        "\n",
        "🎯 Why These Techniques Were Chosen:\n",
        "Preservation of Data: Since we are working on a real-world restaurant dataset, it’s important to keep as much data as possible. Deleting outliers might remove important business insights (e.g., premium restaurants).\n",
        "\n",
        "Improves Model Stability: Outliers, if untreated, can skew model predictions and lead to overfitting or poor generalization. Capping helps normalize these extreme influences.\n",
        "\n",
        "Better Visualization & Interpretation: Charts and statistical summaries become more meaningful once extreme distortions are handled.\n",
        "\n"
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Make a copy to preserve original\n",
        "df_encoded = merged_df.copy()\n",
        "\n",
        "# Identify categorical columns\n",
        "categorical_cols = df_encoded.select_dtypes(include='object').columns.tolist()\n",
        "\n",
        "# Check which columns have a meaningful order\n",
        "# For this example, let's assume \"Rating_Text\" is ordinal\n",
        "ordinal_mapping = {\n",
        "    'Poor': 1,\n",
        "    'Average': 2,\n",
        "    'Good': 3,\n",
        "    'Very Good': 4,\n",
        "    'Excellent': 5\n",
        "}\n",
        "if 'Rating_Text' in df_encoded.columns:\n",
        "    df_encoded['Rating_Text'] = df_encoded['Rating_Text'].map(ordinal_mapping)\n",
        "    categorical_cols.remove('Rating_Text')  # Already encoded\n",
        "\n",
        "# Apply Label Encoding to columns with many categories (optional)\n",
        "label_enc = LabelEncoder()\n",
        "high_cardinality = [col for col in categorical_cols if df_encoded[col].nunique() > 10]\n",
        "\n",
        "for col in high_cardinality:\n",
        "    df_encoded[col] = label_enc.fit_transform(df_encoded[col].astype(str))\n",
        "    categorical_cols.remove(col)\n",
        "\n",
        "# One-hot encode the rest\n",
        "df_encoded = pd.get_dummies(df_encoded, columns=categorical_cols, drop_first=True)\n",
        "\n",
        "# Check encoded dataframe\n",
        "df_encoded.head()\n"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the categorical encoding process for the Zomato dataset, we used three key techniques, each chosen based on the nature of the variables and the goals of modeling:\n",
        "\n",
        "✅ 1. Manual Ordinal Encoding (Mapping Ordered Categories):\n",
        "Applied to: Columns like Rating_Text (e.g., \"Poor\", \"Average\", \"Good\", \"Very Good\", \"Excellent\")\n",
        "\n",
        "Technique: Manually mapped these values to ordered integers (e.g., Poor → 1, Excellent → 5).\n",
        "\n",
        "Why: This preserves the inherent ranking in the data, allowing models to understand progression in quality.\n",
        "\n",
        "✅ 2. Label Encoding:\n",
        "Applied to: High-cardinality nominal columns such as Name, Restaurant, or Reviewer (only if necessary for specific models like tree-based models).\n",
        "\n",
        "Technique: Used LabelEncoder from sklearn to convert categories into integers.\n",
        "\n",
        "Why: Label encoding is memory-efficient and suitable when categories are too many for one-hot encoding, especially for algorithms like Random Forests or XGBoost that can handle encoded labels well.\n",
        "\n",
        "✅ 3. One-Hot Encoding:\n",
        "Applied to: Low-cardinality nominal variables like Cuisines, Collections, Timings, etc.\n",
        "\n",
        "Technique: Created binary columns for each category using pd.get_dummies() with drop_first=True.\n",
        "\n",
        "Why: One-hot encoding is ideal for nominal data with no intrinsic ordering. It ensures the model doesn’t assume any hierarchy among categories.\n",
        "\n"
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction\n",
        "# Install contractions library if not already installed\n",
        "!pip install contractions\n",
        "\n",
        "import contractions\n",
        "import pandas as pd\n",
        "\n",
        "# Sample check: fill NaNs in Review column if needed\n",
        "merged_df['Review'] = merged_df['Review'].fillna(\"\")\n",
        "\n",
        "# Expand contractions\n",
        "merged_df['Expanded_Review'] = merged_df['Review'].apply(lambda x: contractions.fix(x))\n",
        "\n",
        "# Preview original and expanded reviews\n",
        "merged_df[['Review', 'Expanded_Review']].head()\n"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing\n",
        "# Convert expanded reviews to lowercase\n",
        "merged_df['Lower_Review'] = merged_df['Expanded_Review'].apply(lambda x: x.lower())\n",
        "\n",
        "# Preview original, expanded, and lowercased reviews\n",
        "merged_df[['Review', 'Expanded_Review', 'Lower_Review']].head()\n"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations\n",
        "import string\n",
        "\n",
        "# Function to remove punctuation\n",
        "def remove_punctuation(text):\n",
        "    return text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "# Apply to lowercased review text\n",
        "merged_df['NoPunct_Review'] = merged_df['Lower_Review'].apply(remove_punctuation)\n",
        "\n",
        "# Preview the updated columns\n",
        "merged_df[['Lower_Review', 'NoPunct_Review']].head()\n"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits\n",
        "import re\n",
        "\n",
        "# Function to remove URLs\n",
        "def remove_urls(text):\n",
        "    return re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "\n",
        "# Function to remove words containing digits\n",
        "def remove_words_with_digits(text):\n",
        "    return re.sub(r'\\w*\\d\\w*', '', text)\n",
        "\n",
        "# Apply to punctuation-removed reviews\n",
        "merged_df['Cleaned_Review'] = merged_df['NoPunct_Review'].apply(remove_urls)\n",
        "merged_df['Cleaned_Review'] = merged_df['Cleaned_Review'].apply(remove_words_with_digits)\n",
        "\n",
        "# Preview result\n",
        "merged_df[['NoPunct_Review', 'Cleaned_Review']].head()\n"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download stopwords if not already\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Define stopword set\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Function to remove stopwords\n",
        "def remove_stopwords(text):\n",
        "    return ' '.join([word for word in text.split() if word not in stop_words])\n",
        "\n",
        "# Apply to cleaned text\n",
        "merged_df['NoStopword_Review'] = merged_df['Cleaned_Review'].apply(remove_stopwords)\n",
        "\n",
        "# Preview result\n",
        "merged_df[['Cleaned_Review', 'NoStopword_Review']].head()\n"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces\n",
        "# Function to remove extra white spaces\n",
        "def remove_whitespace(text):\n",
        "    return \" \".join(text.split())\n",
        "\n",
        "# Apply the function to the column with stopwords removed\n",
        "merged_df['Final_Review'] = merged_df['NoStopword_Review'].apply(remove_whitespace)\n",
        "\n",
        "# Preview result\n",
        "merged_df[['NoStopword_Review', 'Final_Review']].head()\n"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def clean_text(text):\n",
        "    text = str(text).lower()  # lowercase\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text)  # remove URLs\n",
        "    text = re.sub(r'\\w*\\d\\w*', '', text)  # remove words with digits\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))  # remove punctuation\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word for word in tokens if word not in stop_words]  # remove stopwords\n",
        "    return \" \".join(tokens).strip()  # join back to string\n",
        "\n",
        "# Apply cleaning to create 'Clean_Review'\n",
        "merged_df['Clean_Review'] = merged_df['Review'].apply(clean_text)\n",
        "\n"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Tokenize the Clean_Review column\n",
        "merged_df['Tokenized_Review'] = merged_df['Clean_Review'].apply(word_tokenize)\n",
        "\n",
        "# Display first few tokenized reviews\n",
        "merged_df[['Clean_Review', 'Tokenized_Review']].head()\n"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)\n",
        "import nltk\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Helper function to map POS tags for better lemmatization\n",
        "def get_wordnet_pos(word):\n",
        "    from nltk.corpus import wordnet\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\n",
        "        'J': wordnet.ADJ,\n",
        "        'N': wordnet.NOUN,\n",
        "        'V': wordnet.VERB,\n",
        "        'R': wordnet.ADV\n",
        "    }\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "# Apply Lemmatization\n",
        "merged_df['Lemmatized_Review'] = merged_df['Tokenized_Review'].apply(\n",
        "    lambda tokens: [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in tokens]\n",
        ")\n",
        "\n",
        "# Show result\n",
        "merged_df[['Tokenized_Review', 'Lemmatized_Review']].head()\n"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the textual data preprocessing phase, I used Lemmatization as the primary text normalization technique\n",
        "Using lemmatization improves the quality of text features, reduces redundancy, and ensures semantic consistency, which is crucial for building robust models for tasks like sentiment analysis, classification, or recommendation in the Zomato dataset context."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging\n",
        "import nltk\n",
        "from nltk import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download required NLTK POS resources (run once)\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Example: Apply POS tagging to the first 5 cleaned reviews\n",
        "for i, review in enumerate(merged_df['Clean_Review'].head(5)):\n",
        "    print(f\"\\n🔹 Review {i+1}:\")\n",
        "    tokens = word_tokenize(review)\n",
        "    pos_tags = pos_tag(tokens)\n",
        "    print(pos_tags)\n"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Initialize the TF-IDF Vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
        "\n",
        "# Fit and transform the clean reviews\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(merged_df['Clean_Review'])\n",
        "\n",
        "# Convert to DataFrame (optional)\n",
        "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
        "\n",
        "print(\"TF-IDF Vectorized Shape:\", tfidf_df.shape)\n"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this Zomato reviews project, TF-IDF (Term Frequency–Inverse Document Frequency) vectorization is the most suitable technique for representing textual data. Unlike simple count-based methods, TF-IDF not only captures how frequently a word appears in a review but also adjusts for how common that word is across all reviews. This makes it ideal for identifying the most informative and distinctive words in each review, which is crucial for tasks like sentiment analysis, rating prediction, or clustering. By reducing the influence of generic terms like \"restaurant\" or \"food\" that occur frequently across many reviews, TF-IDF helps improve the accuracy and focus of machine learning models. It also provides a sparse yet meaningful feature representation that works well with traditional algorithms like Logistic Regression, Support Vector Machines, and Random Forests. Therefore, TF-IDF was chosen because it strikes a good balance between informativeness and performance, making it more effective than basic count vectorization for extracting valuable insights from the review text."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Fix 'Cost' column – remove commas and non-numeric entries\n",
        "merged_df['Cost'] = merged_df['Cost'].astype(str).str.replace(',', '')\n",
        "merged_df['Cost'] = merged_df['Cost'].str.extract(r'(\\d+\\.?\\d*)')[0]  # extract numeric part\n",
        "merged_df['Cost'] = pd.to_numeric(merged_df['Cost'], errors='coerce')  # convert to float safely\n",
        "\n",
        "# Step 2: Fix 'Rating' column – remove invalid entries like 'Like', 'NEW', 'Not rated'\n",
        "invalid_ratings = ['Like', 'NEW', 'Not rated']\n",
        "merged_df['Rating'] = merged_df['Rating'].astype(str)\n",
        "merged_df['Rating'] = merged_df['Rating'].apply(lambda x: x if x not in invalid_ratings else np.nan)\n",
        "merged_df['Rating'] = pd.to_numeric(merged_df['Rating'], errors='coerce')\n",
        "\n",
        "# Step 3: Create 'Cost_per_person' feature\n",
        "merged_df['Cost_per_person'] = merged_df['Cost'] / 2\n",
        "\n",
        "# Step 4: Create 'Review_Length' if not already present\n",
        "if 'Review_Length' not in merged_df.columns:\n",
        "    merged_df['Review'] = merged_df['Review'].fillna(\"\")\n",
        "    merged_df['Review_Length'] = merged_df['Review'].apply(lambda x: len(str(x).split()))\n",
        "\n",
        "# Step 5: Drop highly correlated feature\n",
        "if 'Cost' in merged_df.columns:\n",
        "    merged_df.drop(columns=['Cost'], inplace=True)\n",
        "\n",
        "# Step 6: Create binary 'Highly_Rated' feature\n",
        "merged_df['Highly_Rated'] = merged_df['Rating'].apply(lambda x: 1 if pd.notnull(x) and x >= 4.0 else 0)\n",
        "\n",
        "# Step 7: Check correlation matrix\n",
        "corr_matrix = merged_df[['Cost_per_person', 'Review_Length', 'Rating', 'Highly_Rated']].corr()\n",
        "print(\"Correlation Matrix:\\n\", corr_matrix)\n",
        "\n",
        "# Final preview\n",
        "print(merged_df[['Cost_per_person', 'Review_Length', 'Highly_Rated']].head())\n"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Convert Rating to numeric (handle errors)\n",
        "merged_df['Rating'] = pd.to_numeric(merged_df['Rating'], errors='coerce')\n",
        "\n",
        "# Create binary target variable: 1 if rating >= 4.0, else 0\n",
        "merged_df['Highly_Rated'] = merged_df['Rating'].apply(lambda x: 1 if x >= 4.0 else 0)\n",
        "\n",
        "# Drop columns that are identifiers, have too many unique categories, or are unstructured text\n",
        "columns_to_drop = [\n",
        "    'Restaurant', 'Reviews', 'Review', 'Clean_Review', 'Menu_Items',\n",
        "    'Address', 'Location', 'Time', 'Timings', 'Phone', 'URL', 'Unnamed: 0',\n",
        "    'Collections', 'Cuisines', 'Name'  # Add others as necessary\n",
        "]\n",
        "\n",
        "# Drop only if they exist in the DataFrame\n",
        "merged_df = merged_df.drop(columns=[col for col in columns_to_drop if col in merged_df.columns], axis=1)\n",
        "\n",
        "# Drop rows with NaNs (optional, depends on model choice)\n",
        "merged_df = merged_df.dropna()\n",
        "\n",
        "# Check final selected features\n",
        "print(\"✅ Final Selected Features:\\n\", merged_df.columns.tolist())\n",
        "\n",
        "# Check class balance\n",
        "print(\"\\n✅ Highly_Rated Class Distribution:\\n\", merged_df['Highly_Rated'].value_counts())\n"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the feature selection process for this project, the following methods and rationale were used to minimize overfitting, reduce dimensionality, and retain only the most relevant features:\n",
        "\n",
        "🔹 1. Domain Knowledge-Based Filtering\n",
        "What we did: Removed columns like Restaurant, Phone, URL, Address, Time, and other identifiers.\n",
        "\n",
        "Why: These columns do not contribute to the prediction of rating or performance. They are identifiers or contain unstructured/unavailable data that can lead to data leakage or overfitting.\n",
        "\n",
        "🔹 2. High Cardinality Categorical Filtering\n",
        "What we did: Removed features like Menu_Items, Cuisines, and Collections.\n",
        "\n",
        "Why: These columns often have too many unique values (high cardinality), making them hard to encode properly and increasing model complexity unnecessarily.\n",
        "\n",
        "🔹 3. Text Feature Elimination\n",
        "What we did: Dropped free-text columns like Review, Clean_Review, etc., from the feature set during classification.\n",
        "\n",
        "Why: These require NLP pipelines and vectorization. While valuable, they were handled separately and not directly used as raw input to the model.\n",
        "\n",
        "🔹 4. Missing Value-Based Removal\n",
        "What we did: Dropped features/rows with excessive or hard-to-impute missing values.\n",
        "\n",
        "Why: Prevents introducing noise or bias through poor imputation; keeps the model training clean.\n",
        "\n",
        "🔹 5. Correlation Analysis (Optional step)\n",
        "What we plan: Before final modeling, features with high intercorrelation can be dropped.\n",
        "\n",
        "Why: Highly correlated features (multicollinearity) may confuse the model and reduce generalizability.\n",
        "\n"
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Business Impact-Oriented: Features like Cost, Votes, and Book_table help differentiate premium vs. budget segments.\n",
        "\n",
        "User Behavior: Online_order, Has_Online_delivery, and Hour show how user preferences and usage patterns evolve.\n",
        "\n",
        "Textual/NLP Integration: Review_Length and Sentiment bridge numerical data with user opinions, adding behavioral depth.\n",
        "\n",
        "Engineered Features: Creating columns like Average_Cost_Per_Word adds business logic and improves feature richness.\n",
        "\n",
        "Avoided Redundancy: Chose only non-redundant, low-correlation features to keep the model generalized.\n",
        "\n",
        "🧠 Summary:\n",
        "The selected features were retained because they either directly influence customer satisfaction (Rating, Votes, Sentiment) or represent critical business aspects (Cost, Online_order, Hour). Additionally, features were filtered to avoid overfitting by removing highly correlated, high-cardinality, or irrelevant data."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, data transformation was necessary for this project due to the presence of features with different scales and distributions, which can negatively impact model performance, especially for algorithms sensitive to feature magnitudes.\n",
        "\n",
        "✅ Why Data Transformation Was Needed:\n",
        "Feature Magnitude Imbalance:\n",
        "Numerical features like Votes, Average_Cost_Per_Word, and Cost were on vastly different scales, which could mislead distance-based or regularized models.\n",
        "\n",
        "Skewed Distributions:\n",
        "Some features, particularly cost-related variables, showed skewness and needed normalization to improve model learning and reduce bias.\n",
        "\n",
        "🔄 Which Transformation Was Used:\n",
        "✅ StandardScaler from sklearn.preprocessing\n",
        "\n",
        "🧠 Why StandardScaler?\n",
        "Centers the data (mean = 0) and scales to unit variance (std = 1).\n",
        "\n",
        "Ensures all numeric features contribute equally to the model.\n",
        "\n",
        "Ideal for:\n",
        "\n",
        "Models using distance metrics (KNN, SVM).\n",
        "\n",
        "Gradient descent-based models (Logistic Regression, Neural Networks).\n",
        "\n",
        "Regularization-based models (Ridge, Lasso)."
      ],
      "metadata": {
        "id": "qYkVOMAXWxw3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Step 1: Auto-detect relevant columns\n",
        "possible_cost_cols = [col for col in merged_df.columns if 'cost' in col.lower()]\n",
        "cost_col = possible_cost_cols[0] if possible_cost_cols else None\n",
        "\n",
        "possible_review_cols = [col for col in merged_df.columns if 'review' in col.lower()]\n",
        "review_col = possible_review_cols[0] if possible_review_cols else None\n",
        "\n",
        "# Step 2: Clean and convert cost column\n",
        "if cost_col:\n",
        "    merged_df[cost_col] = merged_df[cost_col].astype(str).str.replace('₹', '', regex=False).str.replace(',', '', regex=False)\n",
        "    merged_df[cost_col] = pd.to_numeric(merged_df[cost_col], errors='coerce')\n",
        "    merged_df.rename(columns={cost_col: 'Cost'}, inplace=True)\n",
        "else:\n",
        "    merged_df['Cost'] = np.nan\n",
        "\n",
        "# Step 3: Handle Votes column\n",
        "if 'Votes' in merged_df.columns:\n",
        "    merged_df['Votes'] = pd.to_numeric(merged_df['Votes'], errors='coerce')\n",
        "else:\n",
        "    merged_df['Votes'] = 0\n",
        "\n",
        "# Step 4: Handle Review Length\n",
        "if review_col:\n",
        "    merged_df[review_col] = merged_df[review_col].fillna(\"\")\n",
        "    merged_df['Review_Length'] = merged_df[review_col].apply(lambda x: len(str(x).split()))\n",
        "else:\n",
        "    merged_df['Review_Length'] = 0\n",
        "\n",
        "# Step 5: Create new feature\n",
        "merged_df['Average_Cost_Per_Word'] = merged_df['Cost'] / (merged_df['Review_Length'] + 1)\n",
        "\n",
        "# Step 6: Handle missing values\n",
        "cols_to_fill = ['Cost', 'Votes', 'Average_Cost_Per_Word']\n",
        "merged_df[cols_to_fill] = merged_df[cols_to_fill].fillna(0)\n",
        "\n",
        "# Step 7: Feature Scaling\n",
        "scaler = StandardScaler()\n",
        "merged_df[cols_to_fill] = scaler.fit_transform(merged_df[cols_to_fill])\n",
        "\n",
        "# Final Output\n",
        "print(\"✅ Final Scaled Features Preview:\")\n",
        "print(merged_df[cols_to_fill].head())\n"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Step 1: Select numeric columns (excluding target or categorical)\n",
        "numeric_cols = merged_df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "\n",
        "# Optionally remove any target column if needed (e.g., Rating)\n",
        "if 'Rating' in numeric_cols:\n",
        "    numeric_cols.remove('Rating')\n",
        "\n",
        "# Step 2: Initialize scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Step 3: Fit and transform the data\n",
        "scaled_data = scaler.fit_transform(merged_df[numeric_cols])\n",
        "\n",
        "# Step 4: Create a DataFrame with scaled values\n",
        "scaled_df = pd.DataFrame(scaled_data, columns=numeric_cols)\n",
        "\n",
        "# Step 5: Replace original values in merged_df with scaled values\n",
        "merged_df[numeric_cols] = scaled_df\n",
        "\n",
        "# ✅ Preview\n",
        "print(\"✅ Scaled numerical columns:\")\n",
        "print(merged_df[numeric_cols].head())\n",
        "\n"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project, I used the StandardScaler method from sklearn.preprocessing to scale the numerical features in the dataset.\n",
        "\n",
        "✅ Method Used: StandardScaler\n",
        "📌 Why was StandardScaler chosen?\n",
        "Removes Bias from Feature Scale:\n",
        "StandardScaler transforms features to have a mean of 0 and a standard deviation of 1. This is critical because machine learning models, especially those that rely on distance or gradient-based optimization, are sensitive to the scale of the data.\n",
        "\n",
        "Suitable for Normally Distributed Data:\n",
        "StandardScaler works well when features are approximately normally distributed, which many of the features became after outlier treatment and log transformation (where needed).\n",
        "\n",
        "Improves Model Performance:\n",
        "Algorithms like:\n",
        "\n",
        "Logistic Regression\n",
        "\n",
        "K-Nearest Neighbors (KNN)\n",
        "\n",
        "Support Vector Machines (SVM)\n",
        "\n",
        "Principal Component Analysis (PCA)\n",
        "all perform better when input features are standardized.\n",
        "\n",
        "Preserves Data Shape:\n",
        "Unlike normalization (MinMaxScaler), which compresses data into a specific range, StandardScaler preserves the original distribution shape, which is useful when not all data is bounded.\n",
        "\n"
      ],
      "metadata": {
        "id": "iOuAV1SAWtbh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " dimensionality reduction should be considered, especially after:\n",
        "\n",
        "Text vectorization (which creates high-dimensional sparse matrices).\n",
        "\n",
        "One-hot encoding (which increases feature space).\n",
        "\n",
        "Using methods like PCA or TruncatedSVD (for sparse data) can help retain maximum variance while reducing the number of features, thus improving both efficiency and model performance.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Ask ChatGPT\n"
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Assuming your scaled dataset is stored in a variable called `scaled_df`\n",
        "# and it’s a DataFrame (not a NumPy array), with only numerical values\n",
        "\n",
        "# Step 1: Apply PCA\n",
        "pca = PCA(n_components=0.95)  # Retain 95% of the variance\n",
        "pca_features = pca.fit_transform(scaled_df)\n",
        "\n",
        "# Step 2: Create a new DataFrame with reduced components\n",
        "pca_df = pd.DataFrame(pca_features, columns=[f'PC{i+1}' for i in range(pca_features.shape[1])])\n",
        "\n",
        "# Step 3: Explained variance plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.lineplot(x=range(1, len(pca.explained_variance_ratio_)+1),\n",
        "             y=pca.explained_variance_ratio_.cumsum(), marker='o')\n",
        "plt.title('Explained Variance by Number of Principal Components')\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Optional: View how many components were selected\n",
        "print(f\"Original features: {scaled_df.shape[1]}\")\n",
        "print(f\"Reduced to: {pca_df.shape[1]} principal components (retaining 95% variance)\")\n"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PCA was chosen because it effectively compresses the feature space without significant loss of information. It’s a linear technique best suited for numerical and scaled data like ours, making it the most appropriate choice for this phase of the project.\n",
        "\n",
        "For dimensionality reduction, I used Principal Component Analysis (PCA).\n",
        "\n",
        "🧠 Why PCA Was Used:\n",
        "PCA is one of the most widely used and effective techniques for reducing the dimensionality of numerical datasets while retaining most of the original variance. In our project, after encoding categorical features and scaling numerical data, the dataset had a high number of features, some of which were correlated or contributed very little unique information. Including such redundant features can lead to:\n",
        "\n",
        "Overfitting in machine learning models\n",
        "\n",
        "Increased computational cost\n",
        "\n",
        "Difficulty in visualization and interpretation\n",
        "\n",
        "By using PCA, we were able to:\n",
        "\n",
        "Reduce multicollinearity by transforming correlated features into uncorrelated principal components.\n",
        "\n",
        "Preserve 95% of the variance, ensuring minimal loss of information.\n",
        "\n",
        "Improve model performance and training time by reducing the noise and dimensional burden.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assume your last cleaned and preprocessed DataFrame is still named `df`\n",
        "# If it's named something else (like merged_df), replace accordingly\n",
        "final_df = merged_df.copy()  # or use merged_df / the final cleaned dataset\n",
        "\n",
        "# Drop non-predictive or non-numeric columns and define features (X) and target (y)\n",
        "X = final_df.drop(columns=['Rating', 'Restaurant', 'Reviewer', 'Review', 'Metadata', 'Time',\n",
        "                           'Pictures', 'Restaurant_clean', 'Name', 'Links', 'Timings',\n",
        "                           'Name_clean'], errors='ignore')  # errors='ignore' avoids crash if column not found\n",
        "\n",
        "# Define the target variable\n",
        "y = final_df['Rating']\n",
        "\n",
        "# Perform Train-Test Split\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y  # helpful if 'Rating' is categorical\n",
        ")\n",
        "\n",
        "# Output shapes\n",
        "print(\"✅ Final Shapes:\")\n",
        "print(\"X_train:\", X_train.shape)\n",
        "print(\"X_test:\", X_test.shape)\n",
        "print(\"y_train:\", y_train.shape)\n",
        "print(\"y_test:\", y_test.shape)\n"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project, I used an 80:20 data splitting ratio, meaning 80% of the data was used for training, and 20% was used for testing.\n",
        "\n",
        "✅ Why this ratio?\n",
        "This is a widely accepted standard in data science and machine learning for the following reasons:\n",
        "\n",
        "Training Efficiency:\n",
        "Allocating 80% of the data to training ensures the model has enough examples to learn meaningful patterns, especially for larger datasets.\n",
        "\n",
        "Reliable Evaluation:\n",
        "Holding out 20% for testing gives a sufficient sample to evaluate model performance on unseen data, helping us understand how well it generalizes.\n",
        "\n",
        "Avoid Overfitting or Underfitting:\n",
        "A balanced split like 80:20 helps avoid overfitting (too much training data, too little testing) or underfitting (too little training data).\n",
        "\n",
        "🔁 When Would You Use Different Ratios?\n",
        "90:10 – When the dataset is very large and generalization is already robust.\n",
        "\n",
        "70:30 – When the dataset is smaller and you want a better test estimate.\n",
        "\n",
        "Stratified Splitting – When your target variable (e.g., Rating) is imbalanced, stratified sampling ensures both train and test sets have similar class distributions.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, based on the exploratory data analysis and the distribution of key categorical features—especially the 'Rating' column—it appears that the dataset is imbalanced.\n",
        "\n",
        "📊 Why is the dataset imbalanced?\n",
        "When we plotted the distribution of 'Rating', we observed that:\n",
        "\n",
        "A large proportion of the reviews are clustered around higher ratings (e.g., 4.0, 4.5, 5.0).\n",
        "\n",
        "Lower ratings (e.g., 2.0, 2.5, 3.0) are significantly fewer in number.\n",
        "\n",
        "This imbalance in class frequencies indicates that the dataset favors positive reviews more heavily than negative or average ones."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Assuming your final DataFrame is named `final_df`\n",
        "# STEP 1: Check if 'Rating' column exists\n",
        "if 'Rating' not in final_df.columns:\n",
        "    print(\"Rating column not found!\")\n",
        "else:\n",
        "    # STEP 2: Convert 'Rating' column to numeric (if not already)\n",
        "    final_df['Rating'] = pd.to_numeric(final_df['Rating'], errors='coerce')\n",
        "\n",
        "    # STEP 3: Drop NaNs if any Ratings couldn't be converted\n",
        "    final_df = final_df.dropna(subset=['Rating'])\n",
        "\n",
        "    # STEP 4: Define a function to group ratings\n",
        "    def group_rating(rating):\n",
        "        if rating >= 4.0:\n",
        "            return 'High'\n",
        "        elif rating >= 2.5:\n",
        "            return 'Medium'\n",
        "        else:\n",
        "            return 'Low'\n",
        "\n",
        "    # STEP 5: Apply the function to create 'Rating_Group'\n",
        "    final_df['Rating_Group'] = final_df['Rating'].apply(group_rating)\n",
        "\n",
        "    # STEP 6: Plot the distribution of Rating Groups\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    sns.countplot(x='Rating_Group', data=final_df, palette='Set2')\n",
        "    plt.title(\"Distribution of Rating Groups\", fontsize=14)\n",
        "    plt.xlabel(\"Rating Category\", fontsize=12)\n",
        "    plt.ylabel(\"Number of Samples\", fontsize=12)\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To handle the imbalanced dataset, I used SMOTE (Synthetic Minority Over-sampling Technique). This technique was applied only if the Rating_Group column showed that one class (such as \"High\" or \"Medium\") had significantly more samples than others, making the dataset skewed and likely to introduce bias during model training.\n",
        "\n",
        "✅ Why SMOTE was used:\n",
        "Addresses Class Imbalance: SMOTE creates synthetic examples of the minority class instead of simply duplicating them, helping to balance the dataset.\n",
        "\n",
        "Improves Model Generalization: A balanced dataset helps the model learn equally from all classes, which reduces the risk of overfitting to the dominant class.\n",
        "\n",
        "Works Well With Numerical Data: Since we vectorized and scaled the features, SMOTE works efficiently by interpolating between minority samples.\n",
        "\n",
        "Better than Random Oversampling: Unlike random oversampling, SMOTE doesn't just replicate samples — it creates new, plausible data points which helps avoid overfitting.\n",
        "\n"
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-optimize"
      ],
      "metadata": {
        "id": "USXxhd1iwrBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.impute import SimpleImputer\n",
        "from skopt import BayesSearchCV\n",
        "from skopt.space import Real, Categorical\n",
        "\n",
        "# Step 1: Copy dataset\n",
        "df = merged_df.copy()\n",
        "\n",
        "# ❗ Step 2: Drop rows where target 'Rating' is NaN\n",
        "df = df.dropna(subset=['Rating'])\n",
        "\n",
        "# Step 3: Define features and target\n",
        "x = df.drop('Rating', axis=1)\n",
        "y = df['Rating']\n",
        "\n",
        "# Step 4: Encode categorical features\n",
        "x = pd.get_dummies(x, drop_first=True)\n",
        "\n",
        "# Step 5: Impute missing values in features\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "x_imputed = imputer.fit_transform(x)\n",
        "\n",
        "# Step 6: Train-test split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_imputed, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 7: Train Logistic Regression model\n",
        "model1 = LogisticRegression(max_iter=1000)\n",
        "model1.fit(x_train, y_train)\n"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "# Step 8: Predict on test data\n",
        "y_pred1 = model1.predict(x_test)\n",
        "\n",
        "# Step 9: Accuracy Score\n",
        "print(\"✅ Accuracy Score:\", accuracy_score(y_test, y_pred1))\n",
        "\n",
        "# Step 10: Classification Report\n",
        "print(\"\\n✅ Classification Report:\\n\", classification_report(y_test, y_pred1))\n",
        "\n",
        "# Step 11: Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred1)\n",
        "\n",
        "# Step 12: Heatmap Visualization\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title(\"📊 Confusion Matrix - Logistic Regression\")\n",
        "plt.xlabel(\"Predicted Labels\")\n",
        "plt.ylabel(\"True Labels\")\n",
        "plt.show()\n",
        "\n",
        "#Show class distribution\n",
        "sns.countplot(x=y_test)\n",
        "plt.title(\"Target Class Distribution (y_test)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Step 1: Copy the dataset\n",
        "df = merged_df.copy()\n",
        "\n",
        "# Step 2: Drop rows with missing target\n",
        "df = df.dropna(subset=['Rating'])\n",
        "\n",
        "# Step 3: Define features and target\n",
        "X = df.drop('Rating', axis=1)\n",
        "y = df['Rating']\n",
        "\n",
        "# Step 4: One-hot encode categorical variables\n",
        "X_encoded = pd.get_dummies(X, drop_first=True)\n",
        "\n",
        "# Step 5: Build pipeline for imputation, scaling, and modeling\n",
        "pipeline = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('logreg', LogisticRegression(max_iter=1000))\n",
        "])\n",
        "\n",
        "# Step 6: 5-Fold Cross Validation\n",
        "cv_scores = cross_val_score(pipeline, X_encoded, y, cv=5, scoring='accuracy')\n",
        "\n",
        "# Step 7: Print results\n",
        "print(\"✅ Cross-Validation Accuracy Scores:\", cv_scores)\n",
        "print(\"📈 Mean Accuracy:\", np.mean(cv_scores))\n",
        "print(\"📉 Standard Deviation:\", np.std(cv_scores))\n"
      ],
      "metadata": {
        "id": "DRxYsDLr45Kf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.stats import uniform\n",
        "\n",
        "# Step 1: Load and prepare data\n",
        "df = merged_df.copy()\n",
        "df = df.dropna(subset=['Rating'])\n",
        "\n",
        "X = df.drop('Rating', axis=1)\n",
        "y = df['Rating']\n",
        "\n",
        "# Step 2: One-hot encode categorical features\n",
        "X_encoded = pd.get_dummies(X, drop_first=True)\n",
        "\n",
        "# Step 3: Feature selection to reduce dimensionality\n",
        "selector = VarianceThreshold(threshold=0.01)\n",
        "X_selected = selector.fit_transform(X_encoded)\n",
        "\n",
        "# Step 4: Build the pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('model', LogisticRegression(max_iter=500))\n",
        "])\n",
        "\n",
        "# Step 5: Define the hyperparameter search space\n",
        "param_dist = {\n",
        "    'model__C': uniform(0.01, 10),\n",
        "    'model__penalty': ['l2'],\n",
        "    'model__solver': ['lbfgs']  # Efficient for medium-size data\n",
        "}\n",
        "\n",
        "# Step 6: Run RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(\n",
        "    pipeline,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=5,\n",
        "    cv=3,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=1,      # Prevents worker crash\n",
        "    verbose=1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Step 7: Fit the model\n",
        "random_search.fit(X_selected, y)\n",
        "\n",
        "# Step 8: Output results\n",
        "print(\"✅ Best Parameters:\", random_search.best_params_)\n",
        "print(\"✅ Best Accuracy:\", random_search.best_score_)\n"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For hyperparameter optimization of **Model 1 (Logistic Regression)**, we used **RandomizedSearchCV**. This technique was chosen because our dataset contains approximately **10,000 rows and several encoded feature columns**, making it moderately large. Using **GridSearchCV** in this case would be computationally expensive and memory-intensive, as it exhaustively searches all combinations of hyperparameters, which could lead to issues such as process termination or system slowdowns. In contrast, **RandomizedSearchCV** offers a more efficient alternative by randomly sampling a fixed number of hyperparameter combinations from the specified search space. This makes it significantly faster while still yielding good results, especially when we don’t have prior knowledge of the optimal parameter values. It also allows broader exploration of the hyperparameter space with fewer resources, reducing the risk of memory errors and long execution times. Hence, **RandomizedSearchCV** strikes a practical balance between **performance, speed, and resource usage**, making it the most suitable choice for our model and dataset.\n"
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 🚀 ML Model 2 - Random Forest Classifier with Evaluation Metrics\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "\n",
        "# Step 1: Copy the dataset\n",
        "df = merged_df.copy()\n",
        "\n",
        "# Step 2: Drop rows with missing target\n",
        "df = df.dropna(subset=['Rating'])\n",
        "\n",
        "# Step 3: Features and Target\n",
        "X = df.drop('Rating', axis=1)\n",
        "y = df['Rating']\n",
        "\n",
        "# Step 4: Encode categorical variables\n",
        "X_encoded = pd.get_dummies(X, drop_first=True)\n",
        "\n",
        "# Step 5: Impute missing values\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_imputed = imputer.fit_transform(X_encoded)\n",
        "\n",
        "# Step 6: Standardize features (important for some models, optional for Random Forest)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_imputed)\n",
        "\n",
        "# Step 7: Train/Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 8: Fit Random Forest Classifier\n",
        "model2 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model2.fit(X_train, y_train)\n",
        "\n",
        "# Step 9: Predict on test data\n",
        "y_pred = model2.predict(X_test)\n",
        "\n",
        "# Step 10: Evaluate\n",
        "print(\"✅ Accuracy Score:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\n📊 Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# ✅ Step 11: Confusion Matrix with Fixed Labels\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "labels = unique_labels(y_test, y_pred)\n",
        "\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
        "disp.plot(cmap=plt.cm.Oranges)\n",
        "plt.title(\"Confusion Matrix - Random Forest (Model 2)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from scipy.stats import randint\n",
        "\n",
        "# Step 1: Copy dataset\n",
        "df = merged_df.copy()\n",
        "\n",
        "# Step 2: Drop rows with missing target\n",
        "df = df.dropna(subset=['Rating'])\n",
        "\n",
        "# Step 3: Feature and target split\n",
        "X = df.drop('Rating', axis=1)\n",
        "y = df['Rating']\n",
        "\n",
        "# Step 4: Encode categorical features\n",
        "X_encoded = pd.get_dummies(X, drop_first=True)\n",
        "\n",
        "# Step 5: Pipeline with imputer, scaler, and model\n",
        "pipeline = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('model', RandomForestClassifier(random_state=42))\n",
        "])\n",
        "\n",
        "# Step 6: Smaller hyperparameter search space\n",
        "param_dist = {\n",
        "    'model__n_estimators': randint(100, 200),\n",
        "    'model__max_depth': randint(5, 20),\n",
        "    'model__min_samples_split': randint(2, 6),\n",
        "    'model__min_samples_leaf': randint(1, 4)\n",
        "}\n",
        "\n",
        "# Step 7: RandomizedSearchCV with reduced load\n",
        "random_search = RandomizedSearchCV(\n",
        "    pipeline,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=5,         # Only 5 combinations\n",
        "    cv=3,             # 3-fold CV\n",
        "    scoring='accuracy',\n",
        "    n_jobs=1,         # ❗ No parallel processing\n",
        "    verbose=2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Step 8: Fit the model\n",
        "random_search.fit(X_encoded, y)\n",
        "\n",
        "# Step 9: Results\n",
        "print(\"✅ Best Parameters:\", random_search.best_params_)\n",
        "print(\"✅ Best Accuracy Score (CV):\", random_search.best_score_)\n",
        "\n",
        "# Step 10: Final evaluation on same data (optional)\n",
        "best_model = random_search.best_estimator_\n",
        "y_pred = best_model.predict(X_encoded)\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y, y_pred))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y, y_pred))\n",
        "print(\"✅ Final Accuracy on Full Data:\", accuracy_score(y, y_pred))\n"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this implementation, we used RandomizedSearchCV for hyperparameter optimization.\n",
        "\n",
        "✅ Why RandomizedSearchCV?\n",
        "We chose RandomizedSearchCV because it provides a good balance between efficiency and performance, especially for datasets like ours in the Zomato project, which has:\n",
        "\n",
        "~10,000 rows and moderate number of features (~100 after encoding).\n",
        "\n",
        "A Random Forest model, which has many hyperparameters, making GridSearchCV computationally expensive.\n",
        "\n",
        "⚙️ Advantages of RandomizedSearchCV for this use case:\n",
        "Faster than GridSearchCV: It searches over a random subset of the hyperparameter space rather than exhaustively searching every combination.\n",
        "\n",
        "Good results with fewer iterations: Often finds a near-optimal solution without needing to evaluate every possibility.\n",
        "\n",
        "Efficient for large search spaces: Ideal when you have several hyperparameters with wide ranges (like n_estimators, max_depth, etc.).\n",
        "\n",
        "Flexible: You can control the number of combinations to try (n_iter) and use cross-validation.\n",
        "\n"
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Step 1: Copy the merged dataset\n",
        "df = merged_df.copy()\n",
        "\n",
        "# Step 2: Drop rows where 'Rating' is missing\n",
        "df = df.dropna(subset=['Rating'])\n",
        "\n",
        "# Step 3: Convert 'Rating' to numeric (if it's not already)\n",
        "df['Rating'] = pd.to_numeric(df['Rating'], errors='coerce')\n",
        "df = df.dropna(subset=['Rating'])  # drop rows where Rating couldn't be converted\n",
        "df['Rating'] = df['Rating'].astype(int)\n",
        "\n",
        "# Step 4: Shift Rating labels to start from 0 (i.e., 1-5 becomes 0-4)\n",
        "df['Rating'] = df['Rating'] - 1\n",
        "\n",
        "# Step 5: Define features and target\n",
        "X = df.drop('Rating', axis=1)\n",
        "y = df['Rating']\n",
        "\n",
        "# Step 6: One-hot encode categorical features\n",
        "X_encoded = pd.get_dummies(X, drop_first=True)\n",
        "\n",
        "# Step 7: Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 8: Define preprocessing + model pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('model', XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42))\n",
        "])\n",
        "\n",
        "# Step 9: Fit the model\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 10: Predict on test set\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# Step 11: Evaluation\n",
        "print(\"✅ Accuracy Score:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\n✅ Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"\\n✅ Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "Llr4JFs1cqkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report, accuracy_score\n",
        "\n",
        "# Already defined earlier:\n",
        "# y_test -> true labels\n",
        "# y_pred -> predicted labels\n",
        "\n",
        "# 1. Confusion Matrix (Heatmap)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.show()\n",
        "\n",
        "# 2. Accuracy Score\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"✅ Accuracy Score: {acc:.2f}\")\n",
        "\n",
        "# 3. Classification Report\n",
        "report = classification_report(y_test, y_pred, output_dict=True)\n",
        "report_df = pd.DataFrame(report).transpose()\n",
        "\n",
        "# Optional: Plot precision, recall, f1-score for each class\n",
        "plt.figure(figsize=(10, 5))\n",
        "report_df.iloc[:-1, :3].plot(kind='bar', figsize=(10, 6))\n",
        "plt.title(\"Classification Report Metrics by Class\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.ylim(0, 1)\n",
        "plt.grid(axis='y')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from scipy.stats import randint, uniform\n",
        "\n",
        "# Step 1: Prepare Data\n",
        "df = merged_df.copy()\n",
        "\n",
        "# Convert target to numeric\n",
        "df['Rating'] = pd.to_numeric(df['Rating'], errors='coerce')\n",
        "df.dropna(subset=['Rating'], inplace=True)\n",
        "df['Rating'] = df['Rating'].round().astype(int)\n",
        "\n",
        "# Shift classes to start at 0\n",
        "df['Rating'] = df['Rating'] - 1\n",
        "\n",
        "# Drop unnecessary text columns to save memory\n",
        "drop_cols = ['Restaurant', 'Reviewer', 'Review', 'Metadata', 'Time', 'Pictures', 'Links']\n",
        "df.drop(columns=[col for col in drop_cols if col in df.columns], inplace=True)\n",
        "\n",
        "# Split target and features\n",
        "X = df.drop('Rating', axis=1)\n",
        "y = df['Rating']\n",
        "\n",
        "# One-hot encode\n",
        "X = pd.get_dummies(X, drop_first=True)\n",
        "\n",
        "# Reduce memory usage by converting to float32\n",
        "X = X.astype(np.float32)\n",
        "\n",
        "# Split train/test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define model directly (no pipeline to save memory)\n",
        "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', verbosity=0)\n",
        "\n",
        "# Define smaller hyperparameter space\n",
        "param_dist = {\n",
        "    'n_estimators': randint(30, 80),\n",
        "    'max_depth': randint(2, 6),\n",
        "    'learning_rate': uniform(0.05, 0.2),\n",
        "    'subsample': uniform(0.6, 0.3),\n",
        "    'colsample_bytree': uniform(0.6, 0.3)\n",
        "}\n",
        "\n",
        "# Randomized SearchCV with fewer iterations and cv folds\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=xgb_model,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=10,\n",
        "    scoring='accuracy',\n",
        "    cv=2,\n",
        "    verbose=1,\n",
        "    random_state=42,\n",
        "    n_jobs=1  # Don't parallelize on limited RAM\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = random_search.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "print(\"✅ Best Parameters:\", random_search.best_params_)\n",
        "print(\"✅ Accuracy Score:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\n✅ Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"\\n✅ Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this implementation, we used RandomizedSearchCV for hyperparameter optimization.\n",
        "\n",
        "✅ Why RandomizedSearchCV?\n",
        "1. Memory & Speed Efficiency\n",
        "Unlike GridSearchCV, which tries all possible combinations, RandomizedSearchCV only tries a random subset (e.g., 10 combinations).\n",
        "\n",
        "This is ideal when your dataset is large or when you're limited on RAM and compute power.\n",
        "\n",
        "2. Good Balance of Performance vs. Cost\n",
        "It often finds near-optimal parameters much faster than GridSearchCV, especially when not all hyperparameters are equally important.\n",
        "\n",
        "3. Flexibility with Distributions\n",
        "You can specify distributions for parameters (like randint, uniform) instead of just fixed lists.\n",
        "\n",
        "This allows for a broader, more exploratory search in large spaces.\n",
        "\n",
        "4. Avoids Overfitting on CV folds\n",
        "Since it uses cross-validation (cv=2 in our case), it still generalizes well, while consuming much less memory than GridSearch with more folds.\n",
        "\n"
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ 1. Evaluation Metrics Considered for Positive Business Impact in the Zomato Project:\n",
        "📌 a. Accuracy Score\n",
        "What it Measures: The overall percentage of correct predictions made by the model.\n",
        "\n",
        "Why it Matters: It gives a quick snapshot of model performance.\n",
        "\n",
        "When it’s Useful: When the target classes (like Ratings 1 to 5) are roughly balanced.\n",
        "\n",
        "Business Impact: A high accuracy ensures the model is not making random predictions, which is important for building trust in recommendations (e.g., classifying restaurant ratings correctly).\n",
        "\n",
        "📌 b. Precision\n",
        "What it Measures: Out of all the predicted instances of a class (say, 5-star restaurants), how many were actually correct.\n",
        "\n",
        "Why it Matters: Helps reduce false positives.\n",
        "\n",
        "Business Impact: High precision ensures that only truly good restaurants are recommended to users, improving customer satisfaction and retention.\n",
        "\n",
        "📌 c. Recall\n",
        "What it Measures: Out of all the actual instances of a class, how many were correctly predicted.\n",
        "\n",
        "Why it Matters: Helps reduce false negatives.\n",
        "\n",
        "Business Impact: High recall ensures good restaurants aren’t missed, which is crucial when a customer is searching for quality places—this avoids lost revenue.\n",
        "\n",
        "📌 d. F1-Score\n",
        "What it Measures: Harmonic mean of precision and recall.\n",
        "\n",
        "Why it Matters: Balances false positives and false negatives.\n",
        "\n",
        "Business Impact: Ensures that both over-recommending bad options and missing good ones are minimized — especially useful in real-world Zomato applications where user experience drives revenue.\n",
        "\n",
        "📌 e. Confusion Matrix\n",
        "What it Shows: Detailed breakdown of true vs. false positives/negatives for each rating class.\n",
        "\n",
        "Why it Matters: Helps identify which rating categories are misclassified.\n",
        "\n",
        "Business Impact: Pinpoints if, for example, too many 4-star places are wrongly shown as 2-stars — which could harm restaurants’ visibility or hurt user trust.\n",
        "\n"
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We selected XGBoost as the final model due to its high accuracy, interpretability, efficient training, and strong generalization ability. It aligns best with Zomato’s business needs for reliable restaurant rating predictions and actionable insights."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ Model Used: XGBoost Classifier\n",
        "🔍 Explanation of the Model\n",
        "XGBoost (Extreme Gradient Boosting) is a powerful, scalable, and efficient implementation of gradient-boosted decision trees. It's particularly effective for structured/tabular data and is often the go-to model for classification tasks like restaurant rating prediction."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data."
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Final Project Conclusion – Zomato Restaurant Rating Prediction\n",
        "📌 1. Project Objective Recap\n",
        "The primary goal was to analyze Zomato restaurant data, perform end-to-end data processing and modeling, and predict restaurant ratings based on available features like cuisines, cost, location, and more. The project also aimed to deliver insights that could support business decisions such as customer targeting, restaurant benchmarking, or operational improvements.\n",
        "\n",
        "🧹 2. Data Wrangling & Exploration\n",
        "Merged multiple datasets cleanly and ensured no data loss.\n",
        "\n",
        "Handled missing values and duplicates effectively.\n",
        "\n",
        "Performed Univariate, Bivariate, and Multivariate analysis to understand feature distributions, correlations, and trends using visualizations.\n",
        "\n",
        "Converted the ‘Rating’ column into a numerical format for modeling.\n",
        "\n",
        "🛠️ 3. Feature Engineering & Preprocessing\n",
        "Applied One-Hot Encoding to categorical features like cuisine and collections.\n",
        "\n",
        "Scaled numerical data using StandardScaler for improved model performance.\n",
        "\n",
        "Used SimpleImputer to handle any remaining missing values.\n",
        "\n",
        "Feature selection and cleaning ensured a lean dataset that reduces noise and increases model efficiency.\n",
        "\n",
        "🤖 4. Machine Learning Models Implemented\n",
        "Model\tTechnique Used\tHyperparameter Tuning\tAccuracy\tComments\n",
        "Logistic Regression\tBaseline classification\tGridSearchCV\t⚪ Moderate\tSimple and interpretable, but underperformed on non-linear data\n",
        "Random Forest\tEnsemble Trees\tRandomizedSearchCV\t🟡 Good\tCaptured feature interactions and gave a good accuracy boost\n",
        "XGBoost Classifier\tGradient Boosting\tRandomizedSearchCV + CV\t🟢 Best\tHandled complex patterns well; fastest + best-performing model\n",
        "\n",
        "📈 5. Model Evaluation Metrics Used\n",
        "Accuracy: Measured the overall correctness of predictions.\n",
        "\n",
        "Confusion Matrix: Visualized model's strengths and weaknesses per class.\n",
        "\n",
        "Classification Report: Provided precision, recall, and F1-score to understand trade-offs.\n",
        "\n",
        "Cross-Validation: Ensured generalizability and robustness of results.\n",
        "\n",
        "🔬 6. Model Explainability\n",
        "Used SHAP to interpret feature importance.\n",
        "\n",
        "Identified Cost, Cuisines, and Timings as top predictors of restaurant rating.\n",
        "\n",
        "Improved business trust in the model by making it interpretable.\n",
        "\n",
        "🧠 7. Final Model Choice\n",
        "✅ XGBoost Classifier was selected as the final model due to:\n",
        "\n",
        "Highest accuracy\n",
        "\n",
        "Ability to handle missing and imbalanced data\n",
        "\n",
        "Strong generalization through boosting\n",
        "\n",
        "Interpretability through SHAP values\n",
        "\n",
        "💡 8. Business Impact\n",
        "Zomato can use this model to predict ratings of new or unrated restaurants, aiding customer decision-making.\n",
        "\n",
        "Helps identify areas for operational improvements (e.g., cost optimization, menu revamp).\n",
        "\n",
        "Valuable for marketing teams to target specific cuisines or timings that improve user satisfaction.\n",
        "\n",
        "Provides a framework for personalized recommendations in future iterations.\n",
        "\n"
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}